{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Data Pipeline (Streaming Processing)\n",
        "\n",
        "This notebook defines a Lakeflow Declarative Pipeline (LDP) for streaming processing of customer data, executed by the Databricks bundle resource at `resources/dabs_pipeline.yml`.\n",
        "\n",
        "- Catalog and schema are configured in the pipeline resource and determine where tables are published in Unity Catalog.\n",
        "- Tables materialize directly to `catalog.schema.table` (no `LIVE` schema).\n",
        "- This pipeline processes real-time customer data using streaming sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import LDP and required libraries for streaming\n",
        "import dlt\n",
        "import sys\n",
        "sys.path.append(spark.conf.get(\"bundle.sourcePath\", \".\"))\n",
        "from pyspark.sql.functions import expr, col, to_timestamp, hour, dayofweek, month, year, unix_timestamp, when, round, sum, avg, count, countDistinct, current_timestamp, lit, abs\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
        "\n",
        "# Register FakeDataSource for streaming data generation\n",
        "try:\n",
        "    from pyspark_datasources import FakeDataSource\n",
        "    spark.dataSource.register(FakeDataSource)\n",
        "    print(\"FakeDataSource registered successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"Warning: Could not import FakeDataSource: {e}\")\n",
        "    print(\"Make sure pyspark-data-sources[fake] is installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Data Source: Fake Customer Data\n",
        "\n",
        "This section creates a batch data source using the FakeDataSource to generate synthetic customer data. The data includes customer demographics, purchase behavior, and location information.\n",
        "\n",
        "The batch source generates:\n",
        "- `customer_id`: Unique identifier\n",
        "- `name`: Customer full name\n",
        "- `email`: Email address\n",
        "- `age`: Age between 18-80\n",
        "- `city`: City name\n",
        "- `country`: Country name\n",
        "- `purchase_amount`: Random purchase amount\n",
        "- `product_category`: Product category\n",
        "- `timestamp`: Current timestamp for processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define schema for fake customer data using valid Faker method names\n",
        "# Note: All fields must be StringType as required by pyspark_datasources.fake\n",
        "faker_schema = StructType([\n",
        "    StructField(\"uuid4\", StringType(), True),  # Will be mapped to customer_id\n",
        "    StructField(\"name\", StringType(), True),   # Valid Faker method\n",
        "    StructField(\"email\", StringType(), True),   # Valid Faker method\n",
        "    StructField(\"random_int\", StringType(), True),  # Will be converted to age (int)\n",
        "    StructField(\"city\", StringType(), True),    # Valid Faker method\n",
        "    StructField(\"country\", StringType(), True), # Valid Faker method\n",
        "    StructField(\"pyfloat\", StringType(), True), # Will be converted to purchase_amount (double)\n",
        "    StructField(\"word\", StringType(), True),    # Will be mapped to product_category\n",
        "    StructField(\"date_time\", StringType(), True)  # Will be converted to timestamp\n",
        "])\n",
        "\n",
        "# Create streaming data source using FakeDataSource\n",
        "@dlt.table(\n",
        "    comment=\"Streaming fake customer data from FakeDataSource\"\n",
        ")\n",
        "def customer_stream_raw():\n",
        "    # Create streaming DataFrame using fake data source with valid Faker schema\n",
        "    df_faker = spark.readStream \\\n",
        "        .format(\"fake\") \\\n",
        "        .schema(faker_schema) \\\n",
        "        .option(\"numRows\", 100) \\\n",
        "        .load()\n",
        "    \n",
        "    # Transform Faker data to match our desired customer schema\n",
        "    # Convert string fields to appropriate types\n",
        "    df = df_faker \\\n",
        "        .withColumn(\"customer_id\", col(\"uuid4\")) \\\n",
        "        .withColumn(\"age\", (abs(col(\"random_int\").cast(\"int\")) % 63) + 18) \\\n",
        "        .withColumn(\"purchase_amount\", abs(col(\"pyfloat\").cast(\"double\")) * 500 + 10) \\\n",
        "        .withColumn(\"product_category\", \n",
        "                   when(col(\"word\").rlike(\"^[aeiou]\"), \"Electronics\")\n",
        "                   .when(col(\"word\").rlike(\"^[bcdfg]\"), \"Clothing\")\n",
        "                   .when(col(\"word\").rlike(\"^[hjklm]\"), \"Books\")\n",
        "                   .when(col(\"word\").rlike(\"^[npqr]\"), \"Home & Garden\")\n",
        "                   .otherwise(\"Sports\")) \\\n",
        "        .withColumn(\"timestamp\", to_timestamp(col(\"date_time\"))) \\\n",
        "        .withColumn(\"processing_timestamp\", current_timestamp()) \\\n",
        "        .select(\"customer_id\", \"name\", \"email\", \"age\", \"city\", \"country\", \n",
        "                \"purchase_amount\", \"product_category\", \"timestamp\", \"processing_timestamp\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Silver Layer: Processed Customer Data\n",
        "\n",
        "This section processes the raw streaming customer data with data quality checks and transformations:\n",
        "\n",
        "- Validates customer age (18-80)\n",
        "- Ensures purchase amounts are positive\n",
        "- Adds derived fields for analytics\n",
        "- Applies data quality expectations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Silver table: processed customer data with quality checks\n",
        "@dlt.table(\n",
        "    comment=\"Processed customer data with quality expectations and derived fields\"\n",
        ")\n",
        "@dlt.expect_or_drop(\"valid_age\", \"age >= 18 AND age <= 80\")\n",
        "@dlt.expect_or_drop(\"valid_purchase\", \"purchase_amount > 0\")\n",
        "@dlt.expect(\"non_null_customer_id\", \"customer_id IS NOT NULL\")\n",
        "@dlt.expect(\"non_null_email\", \"email IS NOT NULL\")\n",
        "def customer_stream_silver():\n",
        "    df = dlt.read(\"customer_stream_raw\")\n",
        "    \n",
        "    # Add derived fields for analytics\n",
        "    processed_df = df \\\n",
        "        .withColumn(\"age_group\", \n",
        "                   when(col(\"age\") < 25, \"18-24\")\n",
        "                   .when(col(\"age\") < 35, \"25-34\")\n",
        "                   .when(col(\"age\") < 45, \"35-44\")\n",
        "                   .when(col(\"age\") < 55, \"45-54\")\n",
        "                   .when(col(\"age\") < 65, \"55-64\")\n",
        "                   .otherwise(\"65+\")) \\\n",
        "        .withColumn(\"purchase_tier\",\n",
        "                   when(col(\"purchase_amount\") < 50, \"Low\")\n",
        "                   .when(col(\"purchase_amount\") < 200, \"Medium\")\n",
        "                   .otherwise(\"High\")) \\\n",
        "        .withColumn(\"processing_date\", expr(\"date(processing_timestamp)\")) \\\n",
        "        .withColumn(\"processing_hour\", hour(col(\"processing_timestamp\")))\n",
        "    \n",
        "    return processed_df.filter(col(\"customer_id\").isNotNull() & col(\"email\").isNotNull())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gold Layer: Customer Analytics Aggregations\n",
        "\n",
        "This section creates aggregated metrics from the processed customer streaming data:\n",
        "\n",
        "- **Customer Demographics**: Age group and geographic distribution\n",
        "- **Purchase Analytics**: Revenue metrics by category and tier\n",
        "- **Real-time Metrics**: Hourly processing statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gold table: customer demographics by age group and location\n",
        "@dlt.table(\n",
        "    comment=\"Customer demographics aggregated by age group and location\"\n",
        ")\n",
        "def customer_demographics_gold():\n",
        "    df = dlt.read(\"customer_stream_silver\")\n",
        "    return (\n",
        "        df.groupBy(\"age_group\", \"country\", \"city\")\n",
        "          .agg(\n",
        "              count(\"*\").alias(\"customer_count\"),\n",
        "              round(avg(\"age\"), 1).alias(\"avg_age\"),\n",
        "              round(avg(\"purchase_amount\"), 2).alias(\"avg_purchase_amount\"),\n",
        "              round(sum(\"purchase_amount\"), 2).alias(\"total_revenue\")\n",
        "          )\n",
        "          .orderBy(\"country\", \"city\", \"age_group\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gold table: purchase analytics by product category and tier\n",
        "@dlt.table(\n",
        "    comment=\"Purchase analytics aggregated by product category and purchase tier\"\n",
        ")\n",
        "def purchase_analytics_gold():\n",
        "    df = dlt.read(\"customer_stream_silver\")\n",
        "    return (\n",
        "        df.groupBy(\"product_category\", \"purchase_tier\")\n",
        "          .agg(\n",
        "              count(\"*\").alias(\"transaction_count\"),\n",
        "              round(avg(\"purchase_amount\"), 2).alias(\"avg_purchase_amount\"),\n",
        "              round(sum(\"purchase_amount\"), 2).alias(\"total_revenue\"),\n",
        "              countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
        "          )\n",
        "          .orderBy(\"product_category\", \"purchase_tier\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gold table: real-time processing metrics\n",
        "@dlt.table(\n",
        "    comment=\"Real-time processing metrics by hour and date\"\n",
        ")\n",
        "def realtime_metrics_gold():\n",
        "    df = dlt.read(\"customer_stream_silver\")\n",
        "    return (\n",
        "        df.groupBy(\"processing_date\", \"processing_hour\")\n",
        "          .agg(\n",
        "              count(\"*\").alias(\"records_processed\"),\n",
        "              countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
        "              countDistinct(\"country\").alias(\"countries_count\"),\n",
        "              countDistinct(\"product_category\").alias(\"categories_count\"),\n",
        "              round(sum(\"purchase_amount\"), 2).alias(\"hourly_revenue\"),\n",
        "              round(avg(\"purchase_amount\"), 2).alias(\"avg_purchase_amount\")\n",
        "          )\n",
        "          .orderBy(\"processing_date\", \"processing_hour\")\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
